{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14787201,"sourceType":"datasetVersion","datasetId":9453389}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## dependencies\ndependencies on kaggle are very finicky so these are set up in a specific way","metadata":{}},{"cell_type":"code","source":"!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth\n!pip install bitsandbytes==0.43.2\n!pip uninstall -y torchao\n\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nfrom unsloth.chat_templates import train_on_responses_only\nfrom trl import SFTTrainer, SFTConfig\nimport os, re, datasets, json, torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## build and download the model, trainer, tokenizer, dataset...","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(model_name=\"unsloth/Qwen3-14B-unsloth-bnb-4bit\", max_seq_length=2048, load_in_4bit=True)\n\n# can tweak lora rank r in 2^1 - 2^7\nmodel = FastLanguageModel.get_peft_model(model, r=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha=32, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=3407, use_rslora=False, loftq_config=None)\n\ntokenizer = get_chat_template(tokenizer, chat_template=\"qwen3-instruct\")\n\nloaded_data = json.load(open('/kaggle/input/discord-0209/discord.json'))\ndataset = datasets.Dataset.from_list(loaded_data)\n\n# args.learning_rate, args.max_steps, num_train_epochs\ntrainer = SFTTrainer(model=model, tokenizer=tokenizer, train_dataset=dataset, eval_dataset=None, args=SFTConfig(max_steps=100, num_train_epochs=3, learning_rate=5e-5, dataset_text_field=\"text\", per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=5, logging_steps=1, optim=\"adamw_8bit\", weight_decay=0.001, lr_scheduler_type=\"linear\", seed=3407, report_to=\"none\"))\n\ntrainer = train_on_responses_only(trainer, instruction_part=\"<|im_start|>user\\n\", response_part=\"<|im_start|>assistant\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## test the tokenizer (confirms the dataset is good)","metadata":{}},{"cell_type":"code","source":"print('detokenized whole msg:')\nprint(tokenizer.decode(trainer.train_dataset[67][\"input_ids\"]))\nprint('detokenized just unmasked:')\nprint(tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[67][\"labels\"]]).replace(tokenizer.pad_token, \" \"))\n\n# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\nmax_memory = round(gpu_stats.total_memory / 1024**3, 2)\nprint(f\"gpu = {gpu_stats.name} w/ {max_memory}gb, reserved {start_gpu_memory}gb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()\n\nres_vram, total_vram = torch.cuda.max_memory_reserved() / 1024**3, torch.cuda.get_device_properties(0).total_memory / 1024**3\nprint(f'used {res_vram:.2f} gb vram ({100 * res_vram / total_vram:.2f}% of total)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## it works! (does it?)","metadata":{}},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": \"this is a discord conversation. you are b4444. continue the conversation\"},\n            {\"role\": \"assistant\", \"content\": \"b4444: exclusivity is IMPLIED in common parlance idoit. 'do you want cake or cookies' they mean XOR. if you say both you ARE A GLUTTON.\"},\n            {\"role\": \"user\", \"content\": \"championwastaken: cornball\"}]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n_ = model.generate(**tokenizer(text, return_tensors=\"pt\").to(\"cuda\"), max_new_tokens=200, temperature=0.7, top_p=0.8, top_k=20, streamer=TextStreamer(tokenizer, skip_prompt=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## backup to hugging face","metadata":{}},{"cell_type":"code","source":"!mkdir -p /tmp/qwen\n%cd /tmp/qwen\n# model.save_pretrained_gguf(\"/kaggle/qwen\", tokenizer, quantization_method=\"q4_k_m\")\nmodel.push_to_hub_gguf(\"b44ken/discqwen14b\", tokenizer, quantization_method=\"q4_k_m\", token=\"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\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}